# Winter-attention

This repository contains notes about attention mechanisms with code examples included

**this repo have a inductive bias towards vision transformers** * - *

## Markdowns

- [Introduction to Attention](descriptions/attention.MDttention.MD)

- [Why use Layer Normalization in Transformers?](descriptions/layer_normalization.MD)

## Code Exploration

- [Cosine Similarity and vector similarity](code/vector_similarities.ipynb)

- [Self-attention in MNIST](code/mnist_attention.py)

- [Multi-head attention in MNIST](code/mnist_multihead_attention.py)

- [How to use einsum](code/einsum.ipynb)

- [How to use einops](code/einops.ipynb)

## Vision Transformers

- [What is a Vision Transformer?](vision/Vision%20Transformers.md)

- [Vision Transformer in pytorch from scratch](vision/visionattention.py)

- [Attention and positional encoding visualization](vision/attention%20rollout.ipynb)
